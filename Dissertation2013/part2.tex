\chapter{Получение признаков с использованием нейронных сетей}
\label{chapt2}

Описанный в главе \ref{chapt1} метод получения векторов признаков из столбцов
спектрограммы состоит из нескольких преобразований, каждое из которых опирается
на какие-то свойства музыкальных звукозаписей. Представление спектра звука в
виде вектора признаков необходимо, чтобы облегчить последующую классификацию.
Обучение представлениям -- это раздел машинного обучения, рассматривающий
алгоритмы, направленные на получение наилучших представлений входных данных.
Такие алгоритмы стремятся сохранить наиболее характерные признаки входных данных
в сжатом их представлении.

В основе многих алгоритмов обучения представлениям лежит многослойная нейронная
сеть, при обучении которой используются так называемые методы глубокого
обучения. Важным свойством таких методов является возможность предварительного
обучения каждого слоя нейронной сети в отдельности без учителя, на
неразмеченных данных. Во время предварительного обучения параметры каждого из
слоёв достигают значений, более близких к оптимальным, чем при их случайной
инициализации. Благодаря этому для окончательного обучения нейронной сети в
целом требуется существенно меньше размеченных данных, чем потребовалось бы без
использования предварительного обучения. В обзорной статье Й. Бенджио
\cite{Bengio2009} подробно описываются техники, лежащие в основе методов
глубокого обучения.

Методы обучения представлениям впервые были применены к распознаванию аккордов в
звукозаписях в 2012 году: Хамфри в \cite{Humphrey2012} предложил использовать
свёрточные нейронные сети для получения из спектрограммы признаков, позволяющих
классифицировать звучащий аккорд. Этот тип нейронных сетей успешно применяется
при анализе изображений. Фактически, при распознавании аккордов звуковые данные
первоначально также преобразуются в изображение -- спектрограмму. Также
свёрточные нейронные сети применялись для классификации звуков музыкальных
инструментов \cite{Humphrey2011}.

Многослойные рекуррентные нейронные сети были успешно использованы для
распознавания речи в \cite{Maas2012} и для распознавания аккордов в
\cite{BoulangerLewandowski2013}, где нейронная сеть возвращает на выходе
распознанный аккорд, который при помощи рекуррентного соединения подаётся на
вход на следующем шаге.

Хамфри и др. в \cite{Humphrey2013} отмечают, что из-за своей новизны и сложности
методы обучения представлениям и глубокого обучения только начинают
использоваться в различных задачах музыкального информационного поиска. Эти
методы потенциально могут улучшить внутренние представления звуковых данных,
используемые в различных задачах. При этом, в отличие от изображения, звук
меняется с течением времени, что потенциально может быть использовано при
разработке соответствующих алгоритмов.

В данной работе рассматриваются обычные многослойные (в том числе рекуррентные)
нейронные сети, предварительно обучаемые с помощью очищающих автоассоциаторов. В
разделе \ref{sect2_theory} даётся определение многослойного очищающего
автоассоциатора и сопутствующих понятий. В разделе \ref{sect2_sda} описывается
построение и обучение многослойной нейронной сети с использованием
автоассоциаторов, преобразующей столбец спектрограммы в вектор хроматических
признаков.

\section{Теоретические сведения} \label{sect2_theory}

Определения в этом разделе даны в соответствии с \cite{Vincent2010}.

\emph{Автоассоциатор (автоэнкодер)} представляет из себя пару преобразований:
\begin{equation}
y = f_{\theta}(x) = s(Wx+b)
\end{equation}
\begin{equation} \label{g_theta}
z=g_{\theta'}(y) = s(W'y+b')
\end{equation}
Здесь $x$ -- входной вектор, $z$ -- реконструированный выходной вектор, $y$ --
\emph{внутреннее представление} для $x$, $\theta = \{W,b\}$ и
$\theta'=\{W',b'\}$ -- параметры (обычно накладывают ограничение $W'=W^T$), $s$
-- нелинейная функция активации (обычно это сигмоида или функция
гиперболического тангенса). Иногда в (\ref{g_theta}) выбирают в качестве $s$
линейную функцию. Автоассоциатор удобно представлять в виде нейронной сети с
одним скрытым слоем.

При обучении автоассоциатора минимизируется \emph{функция стоимости} $L(X,
Z(X)))$, где $X$ -- множество всех возможных входных векторов. Чтобы в процессе
обучения преобразования $f_{\theta}(x)$ и $g_{\theta}(y)$ не выродились в
тождественные, накладывают различные ограничения. Часто используемое
ограничение: размерность вектора $y$ должна быть меньше размерности входного
вектора $x$. Другой возможный вариант -- потребовать, чтобы размерность вектора
$y$ была больше размерности вектора $x$ и при этом большинство компонент $y$
были равны 0. При этом $y$ становится разреженным представлением вектора $x$.
Обозначим за $f_{\theta}^j(x)$ $j$-ю компоненту вектора $y$ при данном входном
векторе $x$. Тогда можем определить среднюю величину компонент вектора $y$:
\begin{equation}
\hat{\rho}_j = \frac{1}{m} \sum_{i=1}^m f_{\theta}^j(x^{(i)})
\end{equation}
Чтобы добиться $\hat{\rho}_j = \rho$, где $\rho$ -- параметр, контролирующий
разреженность, добавим слагаемое $L_{\rho}$ в функцию стоимости $L$. Это
слагаемое можно определять разными способами, в рамках данной работы будем
использовать следующую его форму, предложенную в \cite{NgCS294A}:
\begin{equation}
L_{\rho} = \beta \left[ \sum_{j=1}^h \left( \rho \log \frac{\rho}{\hat{\rho}_j} +
(1 - \rho) \log \frac{1 - \rho}{1 - \hat{\rho}_j} \right) \right]
\end{equation}
В дальнейшем будем использовать значение $\rho=0.05$, также в соответствии с
\cite{NgCS294A}.

\emph{Очищающий автоассоциатор} обучается таким образом, чтобы по повреждённому
(зашумлённому) вектору $\tilde{x}$ восстанавливать исходный вектор $x$.
Предполагается, что такие представления более устойчивы к помехам и лучше
отражают внутреннюю структуру входных данных. Показано \cite{Vincent2010}, что
во многих случаях внутренние представления, которые получаются при помощи
очищающего автоассоциатора, позволяют получить лучшие результаты в задачах
классификации по сравнению с представлениями, полученными при помощи обычных
автоассоциаторов. В \cite{Vincent2010} рассматриваются различные способы
получения зашумлённого вектора $\tilde{x}$.

\begin{figure} [htbp] 
  \center
  \includegraphics [scale=0.38] {sda}
  \caption{Многослойный автоассоциатор} 
  \label{img:sda}  
\end{figure}

\begin{figure} [htbp] 
  \center
  \includegraphics [scale=0.38] {sda_n}
  \caption{Многослойная нейронная сеть} 
  \label{img:sda_n}  
\end{figure}

Из автоассоциаторов можно строить многослойные модели, отождествляя нейроны из
скрытого слоя одного автоассоциатора со входными нейронами другого. Пример такой
модели показан на рисунке \ref{img:sda}. В полученной модели слои можно обучать
друг за другом на неразмеченных данных. Значения, полученные в скрытом слое
последнего из автоассоциаторов, могут быть использованы как векторы признаков.
Пример полученной нейронной сети показан на рисунке \ref{img:sda_n}

Рекуррентный автоассоциатор может быть получен из обычного путём добавления
рекуррентных соединений, связывающих выходы скрытого слоя с дополнительными его
входами, по одному дополнительному входу на каждый выход. Фактически, при этом
получается сеть Эльмана, впервые описанная в \cite{Elman1990}. Пример такой
нейронной сети представлен на рисунке \ref{img:rsda}. Промежуточное
представление $y(x_t)$ в таком случае вычисляется как
\begin{equation}
y(x_t) = s(Wx_t + b + Uy(x_{t-1}))
\end{equation}

\begin{figure} [htbp] 
  \center
  \includegraphics [scale=0.38] {rsda}
  \caption{Многослойная нейронная сеть с рекуррентными соединениями} 
  \label{img:rsda}  
\end{figure}

\section{Построение нейронной сети и предобучение при помощи автоассоциаторов}
\label{sect2_sda}

Существенным недостатком автоассоциаторов является невозможность содержательной
интерпретации значений во внутреннем слое. В частности, невозможно построить
шаблонные наборы значений, соответствующие аккордам. Можно попытаться обучить
алгоритм классификации на векторах значений на выходах внутреннего слоя. Но для
случая 25 классов и достаточно большой размерности векторов для обучения такого
классификатора может потребоваться слишком много данных.

Вместо этого соединим внутренный слой с дополнительным слоем, имеющим 12
выходов. Полученную нейронную сеть обучим на размеченных данных таким образом,
чтобы на выходе получались хроматические векторы (как в разделе
\ref{ssect1_chroma}), в которых каждая компонента соответствует одному
тональному классу. На вход этой сети будут подаваться столбцы спектрограммы.
Таким образом, вместо задачи классификации полученная нейронная сеть решает
задачу регрессии. Классификация же полученных 12-мерных векторов делается как
описано в разделе \ref{ssect1_chroma}.

Предварительное обучение слоёв-автоассоциаторов будем производить методом
мини-пакетного (mini-batch) стохастического градиентного спуска. При этом
сначала обучается первый слой на всём обучающем множестве, затем полученные на
его выходе значения используются для обучения второго слоя, и так далее.
Окончательное обучение сети в целом также производится методом мини-пакетного
стохастического градиентного спуска. При этом в качестве целевых векторов
используются 12-мерные бинарные шаблоны аккордов (описанные в параграфе
\ref{ssectL_nn}), а для случая отсутствия аккорда -- нулевой вектор.
При классификации, соответственно, отсутствие аккорда определяется в случае,
когда ни одна из компонент полученного вектора по абсолютной величине не
превосходит некоторого значения $\Delta$, которое подбирается опытным путём.

Для очищающего автоассоциатора возможные помехи на входе можно моделировать
разными способами. В \cite{Vincent2010} выделяются 3 типа помех:
\begin{enumerate}
  \item аддитивный нормально распределенный шум $\tilde{x}|x \sim
  \mathcal{N}(x, \sigma_0^2 I)$, где $\sigma_0$ -- параметр;
  \item маскирующий шум: некоторые случайно выбранные элементы входного вектора
  обнуляются;
  \item шум типа <<соль-и-перец>>: некоторым случайно выбранным элементам
  входного вектора присваиваются значения 0 или 1.
\end{enumerate}
В случае, когда на вход подаются столбцы спектрограммы, очевидно, более
естественными являются помехи первого типа. В параграфе \ref{ssect3_noise}
исследуется влияние различных типов помех на итоговое качество распознавания
аккордов.

Естественным выбором для функции стоимости будет квадрат евклидова расстояния:
$$L(x,z) = ||x-z||^2$$
Во время предварительного обучения расстояние вычисляется между незашумлённым
входным вектором и выходным вектором. Во время окончательного обучения
расстояние вычисляется между шаблоном аккорда и выходным вектором. Для случая
отсутствия аккорда в качестве соответствующего шаблона используется нулевой
вектор.

В случае, когда в обучающей выборке большинство примеров соответствуют только
одному классу, очень вероятно получить в итоге сеть, которая все векторы будет
классифицировать как принадлежащие этому классу. В данном случае имеется 25
возможных классов, и желательно иметь приблизительно одинаковое количество
примеров на каждый класс. Однако не все аккорды используются в музыке одинаково
часто, и, например, аккорд \emph{до-мажор} может встречаться в обучающей выборке
во много раз чаще, чем \emph{фа-диез-мажор}.

Аналогичная проблема встречается и при обучении скрытых марковских моделей и
байесовских сетей для определения последовательности аккордов по
последовательности векторов признаков. В этих моделях часто используют
циклический сдвиг векторов признаков для усреднения параметров, соответствующих
разным аккордам. Этот процесс подробно описан, например, в \cite{Sheh2003}. Идея
его состоит в том, что, поскольку в хроматическом векторе каждая компонента
соответствует одному тональному классу, его циклический сдвиг даёт вектор,
соответствующий аккорду того же типа (мажорный или минорный) с основной нотой,
сдвинутой на полутон.

В данном случае при окончательном обучении нейронной сети в целом можно также
использовать сдвиг. Но входными векторами являются столбцы спектрограммы, и
циклический сдвиг соответствует неестественному переносу высокочастотных
компонент в область низких частот (или наоборот). Циклический сдвиг можно
заменить простым сдвигом окна соответствующего размера по столбцу спектрограммы,
имеющей охват на одну октаву больше.

При помощи такого сдвига из каждого столбца спектрограммы получается 12
различных столбцов, соответствующих 12 аккордам одного типа с разными основными
нотами. Это позволяет уравновесить количество аккордов в пределах одного типа.
Чтобы уравновесить количество аккордов между типами, потребуем, чтобы в процессе
генерации обучающей выборки из спектрограмм разница между общим количеством
мажорных аккордов и общим количеством минорных аккордов не превосходила
заданного числа $H$.

Во время тестирования также можно использовать циклический сдвиг. Для этого
также для каждого столбца спектрограммы генерируется 12 тестовых векторов, для
каждого из которых при помощи нейронной сети получается хроматический вектор.
Для полученных 12 хроматических векторов производятся соответствующие обратные
сдвиги, а в качестве результата берется среднее арифметическое от полученных
векторов. Эффект от такой модификации процесса тестирования рассматривается в
параграфе \ref{ssect3_cycletest}.

Анализу различных конфигураций нейронной сети и подбору наилучших параметров
посвящён параграф \ref{sect3_nn}.

\section{Выводы}

\begin{enumerate}
  \item Предложен метод для получения хроматических векторов из столбцов
  спектрограммы с использованием предварительно обученной многослойной нейронной
  сети.
  \item Предложен способ для создания обучающей выборки, содержащей
  приблизительно одинаковое количество векторов для всех возможных аккордов.
\end{enumerate}

%\newpage
%============================================================================================================================

% А это две картинки под общим номером и названием:
% \begin{figure}[h]
%   \begin{minipage}[h]{0.49\linewidth}
%     \center{\includegraphics[width=0.5\linewidth]{knuth1} \\ а)}
%   \end{minipage}
%   \hfill
%   \begin{minipage}[h]{0.49\linewidth}
%     \center{\includegraphics[width=0.5\linewidth]{knuth2} \\ б)}
%   \end{minipage}
%   \caption{Очень длинная подпись к изображению, на котором представлены две фотографии Дональда Кнута}
%   \label{img:knuth}  
% \end{figure}

%\newpage
%============================================================================================================================

\clearpage